{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FourReal_AV_FinalCombinedModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nupursjsu/AlternusVera/blob/master/FourReal_AV_FinalCombinedModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyEEVfhksrtH",
        "colab_type": "text"
      },
      "source": [
        "#**Alternus Vera - Final Combined Model**\n",
        "\n",
        "Doing multiclass classification between 1 and 6, 1 being true and 6 being pants-fire.\n",
        "\n",
        "We have also performed LDA as part of distillation on liar liar, kaggle fake news and kaggle fake news detection datasets. But to keep this notebook concise we haven't included that here. Link given below.\n",
        "\n",
        "[LDA on datasets](https://drive.google.com/open?id=1eI7Ty9AMRL1Wii4lb7us16gYc88MTkD4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJOxSBxuxfj",
        "colab_type": "text"
      },
      "source": [
        "#**Team Name : FourReal**\n",
        "\n",
        "GitHub URL: https://github.com/nupursjsu/AlternusVera\n",
        "\n",
        "### **Team Contributions:**\n",
        "\n",
        "|Features  |  Member |\n",
        "|-----|-----|\n",
        "| Political Affiliation                         |  Nupur Yadav |  \n",
        "| Title Vs Body                   |  Lokesh Vadlamudi  |  \n",
        "|   Network Factor                           |  Ronak Mehta \n",
        "| Stance Model                 |  Chetan Kulkarni |  \n",
        "\n",
        "\n",
        "### **Enrichment Dataset Details:**\n",
        "\n",
        "- Kaggle Fakenews Dataset\n",
        "- Kaggle News Detection\n",
        "- Politifact Fake news and Real News Content\n",
        "- SemEval Dataset\n",
        "- Fake News Challenge\n",
        "\n",
        "\n",
        "### **Libraries Used:**\n",
        "\n",
        "- NLTK \n",
        "- Gensim \n",
        "- Fastai\n",
        "- Numpy\n",
        "- Pandas\n",
        "- CSV\n",
        "- WordCloud\n",
        "- Seaborn\n",
        "- Scipy\n",
        "- Regualr Expression\n",
        "- Matplotlib\n",
        "- Sklearn "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p30eSCQy9M_",
        "colab_type": "text"
      },
      "source": [
        "**To keep the notebook short and concise we have just imported our already trained models for our individual factors. The links to each individual factors colabs can be found below.**\n",
        "\n",
        "[Political Affiliation](https://drive.google.com/open?id=1UMq9LCX3A20JNMW_bxQl_HRMiTARNrOm)\n",
        "\n",
        "[Network Based](https://drive.google.com/open?id=1nkBr35YVMkWb8JLZeJivl_xQBIXZMWlX)\n",
        "\n",
        "[Title Vs Body](https://drive.google.com/open?id=1g5G8UzV6iZY9wLLechpWpNaE5PzldozH)\n",
        "\n",
        "[Stance Factor](https://drive.google.com/open?id=1PVqhXDSrMVOq6BswfpBz7c6PZCoYS_7x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L7wljCRwY7P",
        "colab_type": "text"
      },
      "source": [
        "#**Importing necessary libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctJLpDlXjrYL",
        "colab_type": "code",
        "outputId": "859695b1-e8b6-4528-f499-f56556762884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip install nltk\n",
        "!pip install gensim\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import pickle\n",
        "from fastai import *\n",
        "from fastai.tabular import *\n",
        "\n",
        "\n",
        "\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "# from nltk.corpus import stopwords\n",
        "# nltk.download('stopwords')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# NLTK libraries\n",
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.16.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soI0dZ-dwMDS",
        "colab_type": "text"
      },
      "source": [
        "#**Feature 1 : Political Affiliation Factor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FcxkVUjjlrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read data file into colaboratory\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBHm6EMMjfkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to load already trained models and make prediction\n",
        "def FourReal_getPartyAffiliationScore(statement, speaker_job_title, state_info, context): # return between 1 and 6, being 1 = true,  6 = pants-fire\n",
        "  if ( (statement == \"\") | (speaker_job_title == \"\") | (state_info == \"\") | (context == \"\")):\n",
        "    return 0\n",
        "\n",
        "  #loading the doc2vec model from drive to generate embeddings for statement\n",
        "  link = 'https://drive.google.com/open?id=1G0GC1VT5ogKyUcMSbdpivDVtPV6Lhheq'\n",
        "  fluff, id = link.split('=')\n",
        "  \n",
        "  #downloading the doc2vec model\n",
        "  downloaded = drive.CreateFile({'id':id}) \n",
        "  downloaded.GetContentFile('doc2vec_model.sav')\n",
        "  d2vmodel = pickle.load(open('doc2vec_model.sav', 'rb'))\n",
        "  \n",
        "  statement_wordvec = [d2vmodel.infer_vector(word_tokenize(statement))]\n",
        "  \n",
        "  #creating the dataframe with our statement embeddings and other categorical columns \n",
        "  #to get a party affiliation probability vector for our final prediction\n",
        "  df = pd.DataFrame(statement_wordvec)\n",
        "  df['speaker_job_title']= speaker_job_title\n",
        "  df['state_info'] = state_info\n",
        "  df['context'] = context\n",
        "  df.columns = ['Embed_0', 'Embed_1', 'Embed_2', 'Embed_3', 'Embed_4', 'Embed_5', 'Embed_6', 'Embed_7','Embed_8', 'Embed_9', 'speaker_job_title', 'state_info', 'context']\n",
        "      \n",
        "  #Loading our NN model to get party affiliation probability vector\n",
        "  link = 'https://drive.google.com/open?id=1i3TknJOHp-kxUuqL9S2nX1jb-3EQVo5F'\n",
        "  fluff, id = link.split('=')\n",
        "  \n",
        "  #downloading the NN model\n",
        "  downloaded = drive.CreateFile({'id':id}) \n",
        "  downloaded.GetContentFile('export.pkl')\n",
        "  NN_model = load_learner('/content')\n",
        "\n",
        "  pred = NN_model.predict(df.iloc[0])\n",
        "  pf_pred = pred[0] \n",
        "  pf_probability_vector = pred[2] #party affiliation probability vector\n",
        "\n",
        "  #creating dataframe for our final prediction between 1-6\n",
        "  #converting vector to dataframe columns\n",
        "  v = []\n",
        "  for i in pf_probability_vector:\n",
        "    a = float(i)\n",
        "    v.append(a)\n",
        "\n",
        "  columns = ['Prob_republican','Prob_democrat', 'Prob_other']\n",
        "  final_df = pd.DataFrame(v).transpose()\n",
        "  final_df.columns = columns\n",
        "\n",
        "  #Loading LR model to make final prediction where a statement lies between 1-6 with 1 being true and 6 being pants-fire\n",
        "  link = 'https://drive.google.com/open?id=1CaBMxljH5Z7FVXude4btyb5KWGoHNDZ2'\n",
        "  fluff, id = link.split('=')\n",
        "  \n",
        "  #downloading the doc2vec model\n",
        "  downloaded = drive.CreateFile({'id':id}) \n",
        "  downloaded.GetContentFile('LogisticRegression.pkl')\n",
        "  pa_model = joblib.load('LogisticRegression.pkl')\n",
        "\n",
        "  predicted = pa_model.predict(final_df)\n",
        "  # predicedProb = pa_model.predict_proba(final_df)[:,1]\n",
        "  predicedProb=pa_model.predict_proba(final_df)\n",
        "  return predicedProb.tolist()[0]     #returns probability vector from 1 to 6, with 1 being true and 6 being pants-fire\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ_MWuFjrvdR",
        "colab_type": "code",
        "outputId": "f198c335-120a-45cb-c87e-917ac9e6c54e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(FourReal_getPartyAffiliationScore(\"Says the Annies List political group supports third-trimester abortions on demand\", \"State representative\",\t\"Texas\", \"a mailer\"))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.18341061950598983, 0.23215116583821807, 0.19679668628356797, 0.16299004866876748, 0.08077991966860482, 0.14387156003485185]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6NhtajeQPVl",
        "colab_type": "text"
      },
      "source": [
        "#**Feature 2 : Title Vs Body**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogL_gq27Gdv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1r2LPuNUKy_qMlLzfDnIVdeVZfZrpOngF'\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "#downloading the doc2vec model\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('lokiXgb.pkl')\n",
        "xgbLoki = joblib.load(open('lokiXgb.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlZTYMVQGneP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1qoqFk7BU_OCMroq_vvcdWDGzm8w-VDmk'\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "#downloading the doc2vec model\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('party.pkl')\n",
        "party = joblib.load(open('party.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUpG8AgUQIOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_tVsBScore(title,body):\n",
        "    # import string\n",
        "    # from nltk.stem import WordNetLemmatizer\n",
        "    # from nltk.tokenize import RegexpTokenizer\n",
        "    # from nltk.corpus import stopwords\n",
        "    # nltk.download('stopwords')\n",
        "    # from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "\n",
        "    stemming = PorterStemmer()\n",
        "    lemma = WordNetLemmatizer()\n",
        "    def cleanUp(data):\n",
        "        data = \"\".join([i for i in str(data) if i not in string.punctuation])\n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        data = tokenizer.tokenize(data.lower())\n",
        "        data = [i for i in data if i not in stopwords.words('english')]\n",
        "        data = [lemma.lemmatize(i) for i in data]\n",
        "        data = \" \".join([stemming.stem(i) for i in data])\n",
        "        # print(data)\n",
        "        return data\n",
        "\n",
        "    # import re\n",
        "    def break_subject(text):\n",
        "        if type(text) is float:\n",
        "          return []\n",
        "        import re\n",
        "        a = re.split('; |, |-',text)\n",
        "        # print(a)\n",
        "        return re.split('; |, |-',text)\n",
        "\n",
        "    cleanedBody = cleanUp(body)\n",
        "    subject = break_subject(title)\n",
        "\n",
        "    from nltk.corpus import wordnet \n",
        "    def generate_Synonyms(wordlist):\n",
        "        synonyms = set()\n",
        "        for i in wordlist:\n",
        "          if type(i) is not str:\n",
        "            continue\n",
        "          for syn in wordnet.synsets(i): \n",
        "              for l in syn.lemmas(): \n",
        "                  synonyms.add(l.name())\n",
        "        # print(list(synonyms))\n",
        "        return list(synonyms)\n",
        "    def combineSynonyms(data):\n",
        "        return ' '.join(data)\n",
        "    \n",
        "\n",
        "    finalSubject = generate_Synonyms(subject)\n",
        "    # print(finalSubject)\n",
        "    finalSubject1 = combineSynonyms(finalSubject)\n",
        "    # print(finalSubject1)\n",
        "\n",
        "    s = cleanedBody + '@@' + finalSubject1\n",
        "    # print(s)\n",
        "\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    # create the transform\n",
        "    def cosine_sim(statement):\n",
        "        statement=statement.split('@@')\n",
        "        # print(statement)\n",
        "        text1,text2=statement[0],statement[1]\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf = vectorizer.fit_transform([text1, text2])\n",
        "        sim_score=((tfidf * tfidf.T).A)[0,1]\n",
        "        pred_label = 0\n",
        "        if sim_score*100>20:\n",
        "          pred_label = 1\n",
        "          \n",
        "        # if sim_score*100>0:\n",
        "        #   return 1\n",
        "        return sim_score , pred_label\n",
        "\n",
        "    import spacy\n",
        "    \n",
        "    # Load English tokenizer, tagger, parser, NER and word vectors\n",
        "    nlp = spacy.load('en')\n",
        "    def spacy_similarity(t1,t2):\n",
        "        doc1 = nlp(t1)\n",
        "        doc2 = nlp(t2)\n",
        "        return doc1.similarity(doc2)\n",
        "\n",
        "    \n",
        "    simScore , label = cosine_sim(s)\n",
        "    simScore2 = spacy_similarity(cleanedBody,finalSubject1)\n",
        "    \n",
        "    return label,simScore2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg2b1ipmISJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def FourReal_TitleVsBody(title,statement,barely_true_counts,false_counts,half_true_counts,mostly_true_counts, pants_on_fire_counts,party_affiliation):\n",
        "  # cosineSim = predict_tVsBScore(title,statement)\n",
        "  party_affiliation_transform = party.transform([party_affiliation])\n",
        "  \n",
        "  df=pd.DataFrame([barely_true_counts])\n",
        "  df.columns=['barely_true_counts']\n",
        "  df['false_counts']=false_counts\n",
        "  df['half_true_counts']=half_true_counts\n",
        "  df['mostly_true_counts']=mostly_true_counts\n",
        "  df['pants_on_fire_counts']=pants_on_fire_counts\n",
        "  df['party affiliation']=party_affiliation_transform\n",
        "  a,df['cosineSim']=predict_tVsBScore(title,statement)\n",
        "  \n",
        "  # barely_true_counts\tfalse_counts\thalf_true_counts\tmostly_true_counts\tpants_on_fire_counts\tparty affiliation\tcosineSim\tlabel\n",
        "  \n",
        "  prediction = xgbLoki.predict(df)\n",
        "  prediction_probability = xgbLoki.predict_proba(df)\n",
        "  return prediction_probability.tolist()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRZcAWUWS_lM",
        "colab_type": "text"
      },
      "source": [
        "#**Feature 3 : Network Factor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Csu7hNyTI3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Google drive link where the dataset is stored.\n",
        "link = 'https://drive.google.com/open?id=1OckRQ_VK1Ljrabnn9J9x7q3x95Du2Vb_'\n",
        "fluff, id = link.split('=')\n",
        "# Download the tsv file.\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('model.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UG-9Y1vTd4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def FourReal_getNetwordBasedScore(text):\n",
        "  text = re.sub(\"[^a-zA-Z]\", \" \", str(text))\n",
        "  text =  text.lower()\n",
        "  text_words = nltk.word_tokenize(text)\n",
        "  stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "  words = [w for w in  text_words  if not w in stops]\n",
        "  wordnet_lem = [WordNetLemmatizer().lemmatize(w) for w in words ]\n",
        "  stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n",
        "  text = \" \".join(stems)\n",
        "  model = tf.keras.models.load_model('model.hdf5')\n",
        "  tokenizer = Tokenizer(num_words=300, lower=True)\n",
        "  tokenizer.fit_on_texts(text)\n",
        "  test_sequences = tokenizer.texts_to_sequences(text)\n",
        "  test_padded_sequences = sequence.pad_sequences(test_sequences, maxlen=300)\n",
        "  result = model.predict_classes(test_padded_sequences)\n",
        "  true,fake = 0,0\n",
        "  total_occurrences = np.size(result)\n",
        "  for i in result:\n",
        "    if i==0:\n",
        "      true+=1\n",
        "    else:\n",
        "      fake+=1\n",
        "  value = fake/total_occurrences\n",
        "  if value<=0.167:\n",
        "    return [0,0,0,0,0,1]\n",
        "  elif value>0.167 and value<=0.334:\n",
        "    return [0,0,0,0,1,0]\n",
        "  elif value>0.334 and value<=0.501:\n",
        "    return [0,0,0,1,0,0]\n",
        "  elif value>0.501 and value<=0.668:\n",
        "    return [0,0,1,0,0,0]\n",
        "  elif value>0.668 and value<=0.835:\n",
        "    return [0,1,0,0,0,0]\n",
        "  else:\n",
        "    return [1,0,0,0,0,0]\n",
        "  # print(\"Probability of text being true:\",(true/total_occurrences),\"Probability of text being fake:\",(fake/total_occurrences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4V6Dg1iThRi",
        "colab_type": "code",
        "outputId": "22be2d7d-37e4-4676-d8b6-5923a5ddeb83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "FourReal_getNetwordBasedScore(\"Says the Annies List political group supports third-trimester abortions on demand.\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpmlUjwBUvJK",
        "colab_type": "text"
      },
      "source": [
        "#**Feature 4 : Stance Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4iva8bwWZ6_",
        "colab_type": "code",
        "outputId": "0c7247f5-f793-4083-b790-013368d16562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from sklearn.externals import joblib \n",
        "import re\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYBpgMGneuvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1Gct0uLHt2x6jVt1SmcqlyyLPGhZu_Hfe'\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "#downloading the doc2vec model\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('le_speaker_job_title_code.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b06FKQrFfA7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1pOEibOMRmNSHUZIHm0wdJfp3SgHRQ4Q2'\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "#downloading the doc2vec model\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('le_speaker.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl0a0UyVfJhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1EqvpovqAxnKlu50oQCj_ryVF5fOxtilW'\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "#downloading the doc2vec model\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('RForest.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nbwwvcDgfsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1Wb8JEYXqYcifnTsMgamv8HaCzP_Vvxp2'\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "#downloading the doc2vec model\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('le_state_info_code.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PITTDlx1fV_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1-SjP9druNndPShP9fSmtibvWel0FGgqP'\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "#downloading the doc2vec model\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('vectorizer.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHhF1Iovfn1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1iY7WghgNCFq8EYStlYNlqqFuRADpjsGa'\n",
        "fluff, id = link.split('=')\n",
        "\n",
        "#downloading the doc2vec model\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('XGB.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9QqTxsuZRaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vector=joblib.load('vectorizer.pkl')\n",
        "loaded_model=joblib.load('XGB.pkl')\n",
        "\n",
        "le_speaker=joblib.load('le_speaker.pkl')\n",
        "le_speaker_job_title_code=joblib.load('le_speaker_job_title_code.pkl')\n",
        "le_state_info_code=joblib.load('le_state_info_code.pkl')\n",
        "model=joblib.load('XGB.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlM6D6reZUZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "vector=joblib.load('vectorizer.pkl')\n",
        "loaded_model=joblib.load('RForest.pkl')\n",
        "def get_stance(statement):\n",
        "  p_statement= re.sub(r\"\\W\", \" \",str(statement))\n",
        "  p_statement=re.sub(r'[,\\.!?]', \" \",str(p_statement))\n",
        "  p_statement=re.sub(r\" \\d+\", \" \",str(p_statement))\n",
        "  p_statement=word_tokenize(p_statement.lower())\n",
        "  p_statement=' '.join(p_statement)\n",
        "  p_statement=' '.join([word for word in p_statement.split() if word not in stop_words])\n",
        "  liar_x = vector.transform([p_statement,])\n",
        "  stance=loaded_model.predict(liar_x)\n",
        "  return stance[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpK4JjPYZp5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def FourReal_getStanceScore(statement,barely_true_counts,false_counts,half_true_counts,mostly_true_counts, pants_on_fire_counts,speaker,state_info,speaker_job_title):\n",
        "  speaker_job_title=le_speaker_job_title_code.transform([speaker_job_title])\n",
        "  speaker=le_speaker.transform([speaker])\n",
        "  state_info=le_state_info_code.transform([state_info])\n",
        "  df=pd.DataFrame([barely_true_counts])\n",
        "  df.columns=['barely_true_counts']\n",
        "  df['false_counts']=false_counts\n",
        "  df['half_true_counts']=half_true_counts\n",
        "  df['mostly_true_counts']=mostly_true_counts\n",
        "\n",
        "  df['pants_on_fire_counts']=pants_on_fire_counts\n",
        "  df['Stance']=get_stance(statement)\n",
        "  df['speaker_code']=speaker\n",
        "  df['state info_code']=state_info\n",
        "  df['speaker_job_title_code']=speaker_job_title\n",
        " \n",
        "  # X=np.array(barely_true_counts,false_counts,half_true_counts,mostly_true_counts, pants_on_fire_counts,Stance,speaker,speaker_job_title,state_info)\n",
        "  # ['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'Stance', 'speaker_code', 'state info_code', 'speaker_job_title_code'] \n",
        "  # ['barely_true_counts', 'pants_on_fire_counts', 'speaker_code', 'mostly_true_counts', 'Stance', 'half_true_counts', 'false_counts', 'state info_code', 'speaker_job_title_code']\n",
        "  prediction=model.predict(df)\n",
        "  prediction_probability=model.predict_proba(df)\n",
        "  return prediction_probability.tolist()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wrbVIpuZsLt",
        "colab_type": "code",
        "outputId": "e315aa6d-2eec-40ce-b8dc-ddcea36ab2b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(FourReal_getStanceScore('i am awsome',10,10,10,10, 10,'dwayne-bohac','Virginia','State representative'))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.1963309645652771, 0.15024597942829132, 0.17514757812023163, 0.17656482756137848, 0.14511354267597198, 0.1565970778465271]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwgYAR5vw9NH",
        "colab_type": "text"
      },
      "source": [
        "#**Final Combined Model**\n",
        "\n",
        "**Since, we are doing a multiclass classification, each of our individual models are generating six probabilities scores for 1 to 6 (1 being true and 6 being pants-fire) class labels and then we are doing a weighted average for every probability and at last returning the max probability with the corresponding index denoting a label from 1-6 (here indices from 0 to 5).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNRZn5ZdT7r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def isFakeNews(statement,subject,speaker_job_title,state_info,context,barely_true_counts,false_counts,half_true_counts,mostly_true_counts, pants_on_fire_counts,speaker,party_affiliation):\n",
        "  accur = [0.22,0.43,0.43,0.95] # using the (normalized) accuracy as weigths\n",
        "  w = [float(i)/sum(accur) for i in accur]\n",
        "  sumW = 0\n",
        "  prob = []\n",
        "  if ( (statement != \"\") & (speaker_job_title != \"\") & (state_info != \"\") & (context != \"\")):\n",
        "    a1,b1,c1,d1,e1,f1 =  FourReal_getPartyAffiliationScore(statement, speaker_job_title, state_info, context)\n",
        "\n",
        "  if ((statement != \"\") & (subject != \"\")):\n",
        "    a2,b2,c2,d2,e2,f2 =  FourReal_TitleVsBody(subject,statement,barely_true_counts,false_counts,half_true_counts,mostly_true_counts, pants_on_fire_counts,party_affiliation)\n",
        "    \n",
        "  if (statement!='' and barely_true_counts!='' and false_counts!='' and half_true_counts!='' and mostly_true_counts!='' and  pants_on_fire_counts!='' and speaker!='' and state_info!='' and speaker_job_title!=''):\n",
        "    a3,b3,c3,d3,e3,f3 =  FourReal_getStanceScore(statement,barely_true_counts,false_counts,half_true_counts,mostly_true_counts, pants_on_fire_counts,speaker,state_info,speaker_job_title)\n",
        "    \n",
        "  if (statement != \"\"):\n",
        "    a4,b4,c4,d4,e4,f4 =  FourReal_getNetwordBasedScore(statement)\n",
        "\n",
        "\n",
        "  A1=a1*w[0]+a2*w[1]+a3*w[2]+a4*w[3] /sum(accur)\n",
        "  A2=b1*w[0]+b2*w[1]+b3*w[2]+b4*w[3] /sum(accur)\n",
        "  A3=c1*w[0]+c2*w[1]+c3*w[2]+c4*w[3] /sum(accur)\n",
        "  A4=d1*w[0]+d2*w[1]+d3*w[2]+d4*w[3] /sum(accur)\n",
        "  A5=e1*w[0]+e2*w[1]+e3*w[2]+e4*w[3] /sum(accur)\n",
        "  A6=f1*w[0]+f2*w[1]+f3*w[2]+f4*w[3] /sum(accur)\n",
        "\n",
        "  prob_list = [A1,A2,A3,A4,A5,A6]\n",
        "  prob_max = max(prob_list)\n",
        "  label = prob_list.index(prob_max)\n",
        "\n",
        "  return label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muLDeEJkb2dO",
        "colab_type": "code",
        "outputId": "d13db210-4e43-4965-9c6f-191de91f1e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result = isFakeNews(\"Hillary Clinton agrees with John McCain by voting to give George Bush the benefit of the doubt on Iran\",\"foreign-policy\",\"President\",\"Illinois\",\"Denver\",70,71,160,163,9,'barack-obama','democrat')\n",
        "print(result)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqjDH7yJkdzs",
        "colab_type": "text"
      },
      "source": [
        "The result is '2' which means 'half-true'"
      ]
    }
  ]
}